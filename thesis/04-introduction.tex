\chapter{Introduction}

\section{Motivation}

Nowadays, human-computer interaction is mainly based on the pointing or typewriter-style devices. 
This kind of interaction can limit the natural ways of manipulation using the hands, which may result in a complication of simple tasks.
One of the overcomplicated control examples is rotating a three-dimensional object.
Using a computer mouse, a user needs to grab the object and rotate it using the mouse, which can only operate in a two-dimensional space. 
The rotation operation represented by the mouse's movement is unintuitive for humans and users need a few attempts to understand how it works. 
In the real world, however, the rotation task is natural thus it is simple how to move hands to rotate the object in a desired way.

Therefore, there exists a need for more natural human-computer interfaces.
One of the proposed approaches involves hand gestures that are interpreted by a computer.
Utilizing hands in a human-computer interface is supported by the fact, that they are even used for non-verbal communication, such as sign or body language.
The another advantage of hands is that manipulative tasks performed using hands in real the world could be interpreted as a series of gestures and used as computer input.

The newest sensors provide data that can be successfully used to recognize gestures and therefore control a computer.
Currently, there are several devices that yield data useful for the gesture recognition. 
An example of such a controller is Microsoft Kinect.
It provides a three-dimensional image of the observed scene, but was designed for applications that interpret the movement of the whole body of the user. 
That is why it lacks the needed accuracy for hand gesture recognition.
 
Another device that is designed to track the movements of a hand and fingers is Leap Motion Controller developed by Leap Motion, Inc. released in July 2013. 
The Leap Motion is a small device, which can be placed in front of a computer. It features extreme finger detection accuracy up to 0.01 mm. 
The controller provides information about a position of each finger and a hand detected in the observed space.
The SDK attached to the device allows to recognize three pre-defined gestures: a circle motion with one finger, a swipe action and tapping on the virtual keys. 
The Leap Motion Controller provides information about every detected hand. This device transmits data with frequency up to 100~Hz. 
Leap Motion Controller is a sensor that can potentially revolutionize the human-computer interactions. 

Currently, there exists no library for gesture recognition that supports the data provided by the Leap Motion Controller.
This is an important limitation when creating Leap Motion-based applications.
Right now, when developers want to create applications utilizing the gesture recognition, they need to operate on low-level, unprocessed data and need to implement the gesture recognition on their own.
This additional work overhead could be minimized using high-level library.

\section{Objectives}

Therefore, the goal of this thesis is to develop the library for gesture recognition dedicated to Leap Motion device, which will help developers to implement applications using gestures as a human-computer interface.

The goal will be achieved by meeting the following objetives:
\begin{itemize}
\item to design the library architecture, 
\item to compare existing methods in the context of hand gesture recognition,
\item to select and implement algorithms for gesture recognition,
\item to implement additional modules enabling the recording and reviewing of gestures saved in a format supported by the library,
\item to create a sample gestures database,
\item to perform gesture recognition tests using Leap Motion Controller.
\end{itemize}
The additional requirement of the library is that the processing should be realized in real time.


The recognition modules are based on Support Vector Machines (SVM) and Hidden Markov Models (HMM). 
SVM is a supervised learning algorithm, which uses set of hyperplanes for non-linearly division of input vectors to different classes. 
HMM is an example of an unsupervised learning algorithm, where model tries to find a hidden structure of data using provided training samples. 
Algorithms used in this thesis belong to a group of machine learning algorithms.

The thesis was developed from October 2013 till January 2014.


\section{Thesis organization}
The thesis is structured in the following manner. 
Chapter~\ref{introGRChapter} contains an overview of the literature concerning the gesture recognition problem. 
This part includes descriptions of gestures taxonomies and state of the art methods used for gesture recognition.
The presentation of Leap Motion Controller is in Chapter~\ref{LMCChapter}.
Chapter~\ref{GestureRecChapter} is devoted to gestures recognizing in the context of Leap Motion Controller.
It contains also descriptions of gestures classification, data representations and additional processing steps for hand gestures recognition using the Leap Motion device. 
The following Chapter~\ref{staticChapter} describes proposed methods, evaluation methodology and experiments for static gesture recognition. 
Chapter~\ref{dynamicChapter} presents a description of dynamic gesture recognition. 
Chapter~\ref{libraryChapter} contains a description of the created library --- its architecture, processes, modules, dependencies on other open-source libraries, and an example of its usage. 
Chapter~\ref{conclusionsChapter} concludes the thesis and provides possible future works. 

\section{Contributions}
Michał Nowicki:
\begin {itemize} 
\item described dynamic gestures (Chapter~\ref{dynamicChapter}),
\item described static gestures (Chapter~\ref{staticChapter} expect Section~\ref{fingerSection}), 
\item implemented code of processing algorithms for static and dynamic gestures recognition without an integration into the library,
\item performed experiments evaluating the static and dynamic gesture recognition. 
\end {itemize}

Olgierd Pilarczyk:
\begin{itemize}
\item wrote the description of Leap Motion controller (Chapter~\ref{LMCChapter}),
\item described data models (Section~\ref{datarepSection}), data preprocessing (Section~\ref{PreprocessingSection}),
\item described library architecture (Section~\ref{architectureSection} expect Subsection~\ref{modelSubsection}, Section~\ref{librariesSection}, Section~\ref{sampleSection}),
\item implemented preprocessing and integration dynamic gesture module to library. 
\end {itemize}

Jakub Wąsikowski:
\begin {itemize}
\item wrote the introduction to gesture recognition (Chapter~\ref{introGRChapter}),
\item described classification of gestures (Section~\ref{classificationSection}) and confusion matrix of static gestures,
\item participated in research and definition of gesture classification,
\item implemented finger recognition, recorder and vizualizer, model and format of gestures recordings, determining the number of clusters for dynamic gestures, class for cross validation, integration static gesture and finger recognition modules to library.
\end{itemize}

Katarzyna Zjawin: 
\begin{itemize}
\item wrote the introduction (Chapter~\ref{introGRChapter}),
\item described finger recognition (Section~\ref{fingerSection}),
\item described the data model (Subsection~\ref{modelSubsection}), processes (Section~\ref{processesSection}), recorder and visualizer (Section~\ref{recordvisualSection}),
\item participated in research and definition of gesture classification,
\item implemented and tested finger recognition,
\item implemented recorder and visualizer, model and format of gestures recordings.
\end{itemize}


